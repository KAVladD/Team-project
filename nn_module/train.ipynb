{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import choices\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils_nn import RandomNDataset, AllDataDataset, init_train_transform, init_val_transform\n",
    "from utils_nn import init_gray_train_transform, init_gray_val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = init_train_transform()\n",
    "val_transform = init_val_transform()\n",
    "\n",
    "gray_train_transform = init_gray_train_transform()\n",
    "gray_val_transform = init_gray_val_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train\"\n",
    "val_path = \"data/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 128\n",
    "\n",
    "# train_dataset = AllDataDataset(train_path, train_transform, n_images=45000) \n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "# val_dataset = AllDataDataset(val_path, val_transform, n_images=15000) \n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset = RandomNDataset(train_path, gray_train_transform, n_images=20000) \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "val_dataset = RandomNDataset(val_path, gray_val_transform, n_images=5000) \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KachnovVA\\Desktop\\test\\.conda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\KachnovVA\\Desktop\\test\\.conda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in resnet18.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Changing the last fc to 4 output features\n",
    "# resnet18.fc = torch.nn.Sequential(torch.nn.Linear(512, 64), torch.nn.Linear(64, 4))\n",
    "resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "resnet18.fc = torch.nn.Linear(512, 1)\n",
    "resnet18 = resnet18.to(device)\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=3e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    print(\"Starting training..\")\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    best_val_loss = 10000\n",
    "    for e in range(0, epochs):\n",
    "        print(\"=\" * 20)\n",
    "        print(f\"Starting epoch {e + 1}/{epochs}\")\n",
    "        print(\"=\" * 20)\n",
    "\n",
    "        if e % 10 == 0 and e != 0:\n",
    "            train_dataloader.dataset.new_data()\n",
    "            val_dataloader.dataset.new_data()\n",
    "\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0  # Not computing val_loss since we'll be evaluating the model multiple times within one epoch\n",
    "\n",
    "        resnet18.train()  # set model to training phase\n",
    "\n",
    "        for train_step, (images, val) in enumerate(train_dataloader):\n",
    "\n",
    "            images = images.to(device)\n",
    "            val = val.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, val)\n",
    "            # Once we get the loss we need to take a gradient step\n",
    "            loss.backward()  # Back propagation\n",
    "            optimizer.step()  # Completes the gradient step by updating all the parameter values (we are using all parameters)\n",
    "            train_loss += (\n",
    "                loss.item()\n",
    "            )  # Loss is a tensor which can't be added to train_loss so .item() converts it to float\n",
    "\n",
    "            # Evaluating the model every 20th step\n",
    "            if train_step % 20 == 0:\n",
    "                print(\"Evaluating at step\", train_step)\n",
    "                val_loss = 0.0\n",
    "\n",
    "                resnet18.eval()  # set model to eval phase\n",
    "\n",
    "                for val_step, (images, val) in enumerate(val_dataloader):\n",
    "\n",
    "                    images = images.to(device)\n",
    "                    val = val.to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = resnet18(images)\n",
    "\n",
    "                    loss = criterion(outputs, val)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                val_loss /= val_step + 1\n",
    "                print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_loss_weights = resnet18.state_dict()\n",
    "\n",
    "                resnet18.train()\n",
    "\n",
    "        train_loss /= train_step + 1\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(\"Training complete..\")\n",
    "    last_loss_weights = resnet18.state_dict()\n",
    "\n",
    "    return last_loss_weights, best_loss_weights, train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..\n",
      "====================\n",
      "Starting epoch 1/100\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KachnovVA\\Desktop\\test\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating at step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KachnovVA\\Desktop\\test\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3228\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1700\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1760\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1602\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1605\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1575\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1554\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KachnovVA\\Desktop\\test\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1320\n",
      "====================\n",
      "Starting epoch 2/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1525\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1510\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1497\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1483\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1477\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1466\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1467\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1447\n",
      "Training Loss: 0.1124\n",
      "====================\n",
      "Starting epoch 3/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1436\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1421\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1423\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1419\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1413\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1400\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1404\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1393\n",
      "Training Loss: 0.1091\n",
      "====================\n",
      "Starting epoch 4/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1384\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1378\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1377\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1364\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1366\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1363\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1366\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1355\n",
      "Training Loss: 0.1077\n",
      "====================\n",
      "Starting epoch 5/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1360\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1344\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1340\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1345\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1338\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1333\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1332\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1326\n",
      "Training Loss: 0.1067\n",
      "====================\n",
      "Starting epoch 6/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1324\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1323\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1318\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1310\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1315\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1308\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1308\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1316\n",
      "Training Loss: 0.1061\n",
      "====================\n",
      "Starting epoch 7/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1312\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1314\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1305\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1295\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1300\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1285\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1297\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1287\n",
      "Training Loss: 0.1057\n",
      "====================\n",
      "Starting epoch 8/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1284\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1287\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1289\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1281\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1278\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1275\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1272\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1271\n",
      "Training Loss: 0.1057\n",
      "====================\n",
      "Starting epoch 9/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1279\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1271\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1271\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1265\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1269\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1263\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1265\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1268\n",
      "Training Loss: 0.1051\n",
      "====================\n",
      "Starting epoch 10/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1265\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1269\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1266\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1260\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1258\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1259\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1260\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1250\n",
      "Training Loss: 0.1050\n",
      "====================\n",
      "Starting epoch 11/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1217\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1228\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1224\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1222\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1213\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1217\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1212\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1212\n",
      "Training Loss: 0.1063\n",
      "====================\n",
      "Starting epoch 12/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1219\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1210\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1207\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1199\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1205\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1208\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1197\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1193\n",
      "Training Loss: 0.1059\n",
      "====================\n",
      "Starting epoch 13/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1196\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1197\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1200\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1193\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1191\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1202\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1190\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1186\n",
      "Training Loss: 0.1057\n",
      "====================\n",
      "Starting epoch 14/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1192\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1193\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1187\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1190\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1195\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1187\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1182\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1188\n",
      "Training Loss: 0.1056\n",
      "====================\n",
      "Starting epoch 15/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1183\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1184\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1178\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1183\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1190\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1176\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1176\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1176\n",
      "Training Loss: 0.1052\n",
      "====================\n",
      "Starting epoch 16/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1167\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1169\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1172\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1177\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1172\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1172\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1167\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1170\n",
      "Training Loss: 0.1052\n",
      "====================\n",
      "Starting epoch 17/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1170\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1167\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1163\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1166\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1163\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1161\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1160\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1163\n",
      "Training Loss: 0.1052\n",
      "====================\n",
      "Starting epoch 18/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1167\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1174\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1154\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1161\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1161\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1162\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1159\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1162\n",
      "Training Loss: 0.1050\n",
      "====================\n",
      "Starting epoch 19/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1171\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1163\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1154\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1153\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1155\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1160\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1159\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1151\n",
      "Training Loss: 0.1050\n",
      "====================\n",
      "Starting epoch 20/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1154\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1158\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1155\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1154\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1160\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1164\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1162\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1159\n",
      "Training Loss: 0.1050\n",
      "====================\n",
      "Starting epoch 21/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1178\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1167\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1168\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1167\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1162\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1168\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1171\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1166\n",
      "Training Loss: 0.1038\n",
      "====================\n",
      "Starting epoch 22/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1164\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1162\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1158\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1160\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1159\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1160\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1158\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1161\n",
      "Training Loss: 0.1036\n",
      "====================\n",
      "Starting epoch 23/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1162\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1160\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1160\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1155\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1159\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1160\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1157\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1153\n",
      "Training Loss: 0.1037\n",
      "====================\n",
      "Starting epoch 24/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1152\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1154\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1157\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1155\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1154\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1156\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1155\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1153\n",
      "Training Loss: 0.1033\n",
      "====================\n",
      "Starting epoch 25/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1153\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1153\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1150\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1148\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1147\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1148\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1144\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1147\n",
      "Training Loss: 0.1034\n",
      "====================\n",
      "Starting epoch 26/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1142\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1143\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1146\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1147\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1147\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1145\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1143\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1143\n",
      "Training Loss: 0.1034\n",
      "====================\n",
      "Starting epoch 27/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1149\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1141\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1145\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1144\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1144\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1143\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1142\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1139\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 28/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1140\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1138\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1139\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1142\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1141\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1140\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1143\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1139\n",
      "Training Loss: 0.1033\n",
      "====================\n",
      "Starting epoch 29/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1142\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1138\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1136\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1139\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1137\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1137\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1136\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1137\n",
      "Training Loss: 0.1034\n",
      "====================\n",
      "Starting epoch 30/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1141\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1141\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1139\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1139\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1140\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1136\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1132\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1135\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 31/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1122\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1127\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1123\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1123\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1120\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1120\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1121\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1123\n",
      "Training Loss: 0.1047\n",
      "====================\n",
      "Starting epoch 32/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1124\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1122\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1123\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1118\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1122\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1125\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1119\n",
      "Training Loss: 0.1048\n",
      "====================\n",
      "Starting epoch 33/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1115\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1119\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1122\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1116\n",
      "Training Loss: 0.1048\n",
      "====================\n",
      "Starting epoch 34/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1127\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1119\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1120\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1116\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1116\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1118\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1118\n",
      "Training Loss: 0.1047\n",
      "====================\n",
      "Starting epoch 35/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1122\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1121\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1116\n",
      "Training Loss: 0.1048\n",
      "====================\n",
      "Starting epoch 36/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1107\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1110\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1120\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1123\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1118\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1118\n",
      "Training Loss: 0.1045\n",
      "====================\n",
      "Starting epoch 37/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1120\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1120\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1118\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1116\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1118\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1111\n",
      "Training Loss: 0.1047\n",
      "====================\n",
      "Starting epoch 38/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1114\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1110\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1111\n",
      "Training Loss: 0.1046\n",
      "====================\n",
      "Starting epoch 39/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1119\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1115\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1111\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1114\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1115\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1113\n",
      "Training Loss: 0.1046\n",
      "====================\n",
      "Starting epoch 40/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1119\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1116\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1115\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1111\n",
      "Training Loss: 0.1045\n",
      "====================\n",
      "Starting epoch 41/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1118\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1108\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1116\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1123\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1115\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1118\n",
      "Training Loss: 0.1033\n",
      "====================\n",
      "Starting epoch 42/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1110\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1124\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1111\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1106\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1109\n",
      "Training Loss: 0.1033\n",
      "====================\n",
      "Starting epoch 43/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1105\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1109\n",
      "Training Loss: 0.1033\n",
      "====================\n",
      "Starting epoch 44/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1110\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1111\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1114\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1110\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1111\n",
      "Training Loss: 0.1031\n",
      "====================\n",
      "Starting epoch 45/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1105\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1108\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1115\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1108\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1106\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1111\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 46/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1119\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1105\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1110\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1107\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1096\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1105\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1109\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 47/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1105\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1107\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1107\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1112\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 48/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1108\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1114\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1110\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1117\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1116\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1105\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1109\n",
      "Training Loss: 0.1031\n",
      "====================\n",
      "Starting epoch 49/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1106\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1110\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1114\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1112\n",
      "Training Loss: 0.1031\n",
      "====================\n",
      "Starting epoch 50/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1112\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1119\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1108\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1106\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1111\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1114\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1112\n",
      "Training Loss: 0.1033\n",
      "====================\n",
      "Starting epoch 51/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1134\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1129\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1136\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1131\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1142\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1122\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1130\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1146\n",
      "Training Loss: 0.1063\n",
      "====================\n",
      "Starting epoch 52/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1126\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1134\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1135\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1128\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1121\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1131\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1132\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1129\n",
      "Training Loss: 0.1063\n",
      "====================\n",
      "Starting epoch 53/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1129\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1129\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1134\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1130\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1133\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1123\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1128\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1131\n",
      "Training Loss: 0.1065\n",
      "====================\n",
      "Starting epoch 54/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1126\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1125\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1129\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1135\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1124\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1128\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1125\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1121\n",
      "Training Loss: 0.1065\n",
      "====================\n",
      "Starting epoch 55/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1130\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1125\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1133\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1119\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1119\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1125\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1146\n",
      "Training Loss: 0.1062\n",
      "====================\n",
      "Starting epoch 56/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1126\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1128\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1120\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1124\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1127\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1127\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1126\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1124\n",
      "Training Loss: 0.1064\n",
      "====================\n",
      "Starting epoch 57/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1126\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1122\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1134\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1125\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1137\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1126\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1129\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1124\n",
      "Training Loss: 0.1066\n",
      "====================\n",
      "Starting epoch 58/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1119\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1129\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1122\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1125\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1130\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1124\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1129\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1131\n",
      "Training Loss: 0.1061\n",
      "====================\n",
      "Starting epoch 59/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1125\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1130\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1128\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1139\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1128\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1125\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1122\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1132\n",
      "Training Loss: 0.1064\n",
      "====================\n",
      "Starting epoch 60/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1133\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1129\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1132\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1126\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1133\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1130\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1130\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1126\n",
      "Training Loss: 0.1064\n",
      "====================\n",
      "Starting epoch 61/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1118\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1105\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1107\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1113\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1107\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1104\n",
      "Training Loss: 0.1042\n",
      "====================\n",
      "Starting epoch 62/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1107\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1114\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1101\n",
      "Training Loss: 0.1041\n",
      "====================\n",
      "Starting epoch 63/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1102\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1109\n",
      "Training Loss: 0.1043\n",
      "====================\n",
      "Starting epoch 64/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1106\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1110\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1105\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1106\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1100\n",
      "Training Loss: 0.1042\n",
      "====================\n",
      "Starting epoch 65/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1097\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1096\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1102\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1107\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1099\n",
      "Training Loss: 0.1042\n",
      "====================\n",
      "Starting epoch 66/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1096\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1097\n",
      "Training Loss: 0.1042\n",
      "====================\n",
      "Starting epoch 67/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1094\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1097\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1101\n",
      "Training Loss: 0.1042\n",
      "====================\n",
      "Starting epoch 68/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1105\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1095\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1091\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1095\n",
      "Training Loss: 0.1041\n",
      "====================\n",
      "Starting epoch 69/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1111\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1094\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1092\n",
      "Training Loss: 0.1040\n",
      "====================\n",
      "Starting epoch 70/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1093\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1102\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1097\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1094\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1100\n",
      "Training Loss: 0.1039\n",
      "====================\n",
      "Starting epoch 71/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1060\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1072\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1065\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1067\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1067\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1053\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1064\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1060\n",
      "Training Loss: 0.1038\n",
      "====================\n",
      "Starting epoch 72/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1067\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1053\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1059\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1061\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1067\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1059\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1057\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1054\n",
      "Training Loss: 0.1038\n",
      "====================\n",
      "Starting epoch 73/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1055\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1066\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1056\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1055\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1061\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1059\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1065\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1066\n",
      "Training Loss: 0.1037\n",
      "====================\n",
      "Starting epoch 74/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1057\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1061\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1055\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1063\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1058\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1053\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1056\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1051\n",
      "Training Loss: 0.1038\n",
      "====================\n",
      "Starting epoch 75/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1056\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1060\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1059\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1060\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1060\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1076\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1058\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1053\n",
      "Training Loss: 0.1038\n",
      "====================\n",
      "Starting epoch 76/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1046\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1058\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1051\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1066\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1054\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1046\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1060\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1065\n",
      "Training Loss: 0.1038\n",
      "====================\n",
      "Starting epoch 77/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1055\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1061\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1057\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1056\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1057\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1057\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1073\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1057\n",
      "Training Loss: 0.1040\n",
      "====================\n",
      "Starting epoch 78/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1049\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1059\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1048\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1053\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1063\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1064\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1057\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1047\n",
      "Training Loss: 0.1038\n",
      "====================\n",
      "Starting epoch 79/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1056\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1058\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1054\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1063\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1064\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1056\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1048\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1058\n",
      "Training Loss: 0.1038\n",
      "====================\n",
      "Starting epoch 80/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1052\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1070\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1051\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1057\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1058\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1057\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1058\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1055\n",
      "Training Loss: 0.1038\n",
      "====================\n",
      "Starting epoch 81/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1116\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1097\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1108\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1111\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1102\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 82/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1097\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1109\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1096\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1096\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1104\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 83/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1097\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1102\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1094\n",
      "Training Loss: 0.1033\n",
      "====================\n",
      "Starting epoch 84/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1097\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1096\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1095\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1092\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1094\n",
      "Training Loss: 0.1030\n",
      "====================\n",
      "Starting epoch 85/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1090\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1100\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 86/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1103\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1102\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1107\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1107\n",
      "Training Loss: 0.1033\n",
      "====================\n",
      "Starting epoch 87/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1095\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1104\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1097\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1102\n",
      "Training Loss: 0.1031\n",
      "====================\n",
      "Starting epoch 88/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1096\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1094\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1097\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1101\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 89/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1096\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1101\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1096\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1092\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1093\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1094\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1096\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 90/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1095\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1098\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1095\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1100\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1095\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1099\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1092\n",
      "Training Loss: 0.1032\n",
      "====================\n",
      "Starting epoch 91/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1087\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1084\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1086\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1075\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1092\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1083\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1083\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1087\n",
      "Training Loss: 0.1030\n",
      "====================\n",
      "Starting epoch 92/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1088\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1082\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1091\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1082\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1079\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1084\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1089\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1080\n",
      "Training Loss: 0.1030\n",
      "====================\n",
      "Starting epoch 93/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1080\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1086\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1082\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1077\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1076\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1076\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1083\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1081\n",
      "Training Loss: 0.1031\n",
      "====================\n",
      "Starting epoch 94/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1087\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1082\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1086\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1084\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1080\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1072\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1076\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1087\n",
      "Training Loss: 0.1030\n",
      "====================\n",
      "Starting epoch 95/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1082\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1083\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1084\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1078\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1075\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1079\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1082\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1082\n",
      "Training Loss: 0.1030\n",
      "====================\n",
      "Starting epoch 96/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1076\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1074\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1086\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1075\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1074\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1083\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1076\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1083\n",
      "Training Loss: 0.1031\n",
      "====================\n",
      "Starting epoch 97/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1082\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1080\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1078\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1085\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1079\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1073\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1080\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1072\n",
      "Training Loss: 0.1029\n",
      "====================\n",
      "Starting epoch 98/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1073\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1074\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1073\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1075\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1079\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1078\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1076\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1073\n",
      "Training Loss: 0.1030\n",
      "====================\n",
      "Starting epoch 99/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1075\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1075\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1074\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1080\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1074\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1074\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1080\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1089\n",
      "Training Loss: 0.1031\n",
      "====================\n",
      "Starting epoch 100/100\n",
      "====================\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1083\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.1077\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1081\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1079\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1081\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.1073\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.1081\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.1072\n",
      "Training Loss: 0.1030\n",
      "Training complete..\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_n = 100\n",
    "exp_name = \"gray_ep_100_angle_new_data_lr_6\"\n",
    "\n",
    "last_loss_weights, best_loss_weights, train_loss_list, val_loss_list = train(epochs=epoch_n)\n",
    "\n",
    "save_path = os.path.join(\"weights\", exp_name)\n",
    "try: \n",
    "    os.mkdir(save_path)\n",
    "except: \n",
    "    pass\n",
    "\n",
    "torch.save(last_loss_weights, save_path+\"/last.pth\")\n",
    "torch.save(last_loss_weights, save_path+\"/best.pth\")\n",
    "np.save(save_path+\"/train_loss.npy\", train_loss_list)\n",
    "np.save(save_path+\"/val_loss.npy\", val_loss_list)\n",
    "\n",
    "plt.plot(train_loss_list)\n",
    "plt.plot(val_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(epochs):\n",
    "#     print('Starting training..')\n",
    "#     for e in range(0, epochs):\n",
    "#         print('='*20)\n",
    "#         print(f'Starting epoch {e + 1}/{epochs}')\n",
    "#         print('='*20)\n",
    "\n",
    "#         train_loss = 0.\n",
    "#         val_loss = 0.  # Not computing val_loss since we'll be evaluating the model multiple times within one epoch\n",
    "        \n",
    "        \n",
    "#         resnet18.train() # set model to training phase\n",
    "        \n",
    "#         for train_step, (images, val) in enumerate(train_dataloader):\n",
    "            \n",
    "#             images = images.to(device)\n",
    "#             val = val.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = resnet18(images)\n",
    "#             loss = criterion(outputs, val)\n",
    "#             # Once we get the loss we need to take a gradient step\n",
    "#             loss.backward() # Back propagation\n",
    "#             optimizer.step() # Completes the gradient step by updating all the parameter values (we are using all parameters)\n",
    "#             train_loss += loss.item() # Loss is a tensor which can't be added to train_loss so .item() converts it to float\n",
    "            \n",
    "#             # Evaluating the model every 20th step\n",
    "#             if train_step % 20 == 0:\n",
    "#                 print('Evaluating at step', train_step)\n",
    "\n",
    "#                 accuracy = 0\n",
    "\n",
    "#                 resnet18.eval() # set model to eval phase\n",
    "\n",
    "#                 for val_step, (images, val) in enumerate(val_dataloader):\n",
    "                    \n",
    "#                     images = images.to(device)\n",
    "#                     val = val.to(device)\n",
    "                    \n",
    "#                     with torch.no_grad():\n",
    "#                         outputs = resnet18(images)\n",
    "                        \n",
    "#                     loss = criterion(outputs, val)\n",
    "#                     val_loss += loss.item()\n",
    "\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "#                     accuracy += sum((preds.cpu() == val.cpu()).numpy()) # adding correct preds to acc\n",
    "\n",
    "#                 val_loss /= (val_step + 1)\n",
    "#                 accuracy = accuracy/len(val_dataset)\n",
    "#                 print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "#                 resnet18.train()\n",
    "\n",
    "#                 # if accuracy >= 0.85:\n",
    "#                 if accuracy >= 0.9:\n",
    "#                     print('Performance condition satisfied, stopping..')\n",
    "#                     return\n",
    "\n",
    "#         train_loss /= (train_step + 1)\n",
    "\n",
    "#         print(f'Training Loss: {train_loss:.4f}')\n",
    "#     print('Training complete..')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
